---
output: github_document
always_allow_html: true
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%")
```

# contentanalysis

<!-- badges: start -->
<!-- badges: end -->

`contentanalysis` provides comprehensive tools for extracting and analyzing scientific content from PDF documents, including citation extraction, reference matching, text analysis, and bibliometric indicators.

## Features

- **PDF Import**: Multi-column layout support with structure preservation
- **Citation Extraction**: Comprehensive detection of citation formats (numbered, author-year, narrative, parenthetical)
- **Reference Parsing**: Extract references from text or CrossRef API
- **Citation-Reference Matching**: Automatic matching with multiple disambiguation strategies
- **Text Analysis**: Word frequencies, n-grams, lexical diversity
- **Citation Context**: Extract surrounding text for each citation
- **Bibliometric Indicators**: Citation density, distribution by section, co-occurrence networks
- **Word Distribution**: Track word frequencies across document sections

## Installation

You can install the development version from GitHub:
```r
# install.packages("devtools")
devtools::install_github("yourusername/contentanalysis")
```

## Example

Complete workflow analyzing a real scientific paper:
```{r}
library(contentanalysis)
```

### Download example paper (open access)

```{r}
paper_url <- "https://raw.githubusercontent.com/massimoaria/contentanalysis/master/inst/examples/example_paper.pdf"
download.file(paper_url, destfile = "example_paper.pdf", mode = "wb")
```

### Import PDF with automatic section detection
```{r}
doc <- pdf2txt_auto("example_paper.pdf", n_columns = 2)

# Check detected sections
names(doc)
```


### Perform comprehensive content analysis with CrossRef
```{r}
analysis <- analyze_scientific_content_enhanced(
  text = doc,
  doi = "10.1016/j.mlwa.2021.100094",
  mailto = "your@email.com"
)
```

### View summary statistics
```{r}

analysis$summary
```

### Examine citations by type
```{r}
analysis$citation_metrics$type_distribution
```

### Check matching quality
```{r}
print_matching_diagnostics(analysis)
```

### Analyze citation contexts
```{r}
head(analysis$citation_contexts[, c("citation_text_clean", "section", "full_context")])
```

### Track methodological terms across sections
```{r}
method_terms <- c("machine learning", "regression", "validation", "dataset")
word_dist <- calculate_word_distribution(doc, method_terms)
```

### Create interactive visualization
```{r eval=FALSE, include=FALSE}
plot_word_distribution(word_dist, plot_type = "line", show_points = TRUE)
```

### Examine most frequent words
```{r}
head(analysis$word_frequencies, 10)
```

### Citation co-occurrence network
```{r}
head(analysis$network_data)
```

### Working with references
```{r}
# View parsed references
head(analysis$parsed_references[, c("ref_first_author", "ref_year", "ref_full_text")])

# Find citations to specific author
library(dplyr)
analysis$citation_references_mapping %>%
  filter(grepl("Smith", ref_authors, ignore.case = TRUE))

# Citations by section
analysis$citation_metrics$section_distribution
```

### Advanced: Word distribution analysis
```{r}
# Track disease-related terms
disease_terms <- c("covid", "pandemic", "health", "policy", "vaccination")
dist <- calculate_word_distribution(doc, disease_terms, use_sections = TRUE)

# View frequencies by section
dist %>%
  select(segment_name, word, count, percentage) %>%
  arrange(segment_name, desc(percentage))

# Visualize trends
#plot_word_distribution(dist, plot_type = "area", smooth = FALSE)
```

## Main Functions

- `pdf2txt_auto()`: Import PDF with automatic section detection
- `analyze_scientific_content_enhanced()`: Comprehensive content and citation analysis
- `parse_references_section()`: Parse reference list
- `match_citations_to_references()`: Match citations to references
- `calculate_word_distribution()`: Track word frequencies across sections
- `plot_word_distribution()`: Interactive visualization of word distribution

## Dependencies

Core: pdftools, dplyr, tidyr, stringr, tidytext, tibble, httr2

Suggested: plotly, RColorBrewer, scales (for visualization)

## Citation

If you use this package in your research, please cite:
```
Massimo Aria (2025). contentanalysis: Scientific Content and Citation Analysis from PDF Documents.
R package version 0.1.0.
```
