---
title: "Introduction to contentanalysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to contentanalysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = TRUE  # Set to TRUE if you want to run the examples
)
```

```{r setup}
library(contentanalysis)
library(dplyr)
```

## Introduction

The `contentanalysis` package provides comprehensive tools for
extracting and analyzing scientific content from PDF documents. This
vignette demonstrates the main features using a real open-access
scientific paper.

## Getting Started

### Download Example Paper

We'll use an open-access paper on Machine Learning with Applications:

```{r download}
# Download example paper
paper_url <- "https://raw.githubusercontent.com/massimoaria/contentanalysis/master/inst/examples/example_paper.pdf"
download.file(paper_url, destfile = "example_paper.pdf", mode = "wb")
```

## PDF Import and Section Detection

### Basic Import

```{r import-basic}
# Import with automatic section detection
doc <- pdf2txt_auto("example_paper.pdf", n_columns = 2, citation_type = "author_year")

# Check what sections were detected
names(doc)
```

The function automatically detects common academic sections like
Abstract, Introduction, Methods, Results, Discussion, etc.

### Manual Column Specification

For papers with specific layouts:

```{r import-manual}
# Single column
# doc_single <- pdf2txt_auto("example_paper.pdf", n_columns = 2)

# Three columns
# doc_three <- pdf2txt_auto("example_paper.pdf", n_columns = 3)

# Without section splitting
# text_only <- pdf2txt_auto("example_paper.pdf", sections = FALSE)
```

## Comprehensive Content Analysis

### Full Analysis with CrossRef Integration

```{r analysis}
analysis <- analyze_scientific_content(
  text = doc,
  doi = "10.1016/j.mlwa.2021.100094",        # Optional but recommended
  mailto = "your@email.com",                 # Required for CrossRef,
  citation_type = "author_year",             # Citation style  
  window_size = 10,                          # Words around citations
  remove_stopwords = TRUE,
  ngram_range = c(1, 3),
  use_sections_for_citations = TRUE
)
```

### Understanding the Results

The analysis object contains multiple components:

```{r results-structure}
names(analysis)
#> [1] "text_analytics"              "citations"                  
#> [3] "citation_contexts"           "citation_metrics"           
#> [5] "citation_references_mapping" "parsed_references"          
#> [7] "word_frequencies"            "ngrams"                     
#> [9] "network_data"                "section_colors"
#> [11] "summary"
```

### Summary Statistics

```{r summary}
analysis$summary
```

Key metrics include: - Total words analyzed - Number of citations
extracted - Citation types (narrative vs. parenthetical) - Number of
references matched - Lexical diversity - Citation density per 1000 words

## Citation Analysis

### Citation Extraction

The package detects multiple citation formats:

```{r citations}
# View all citations
head(analysis$citations)

# Citation types found
table(analysis$citations$citation_type)

# Citations by section
analysis$citation_metrics$section_distribution
```

### Citation Contexts

Extract the text surrounding each citation:

```{r contexts}
# View citation contexts
contexts <- analysis$citation_contexts %>%
  select(citation_text_clean, section, words_before, words_after)

head(contexts)

# Find citations in Introduction
intro_citations <- analysis$citation_contexts %>%
  filter(section == "Introduction")

nrow(intro_citations)
```

## Citation Network Visualization

The package can create interactive network visualizations showing how citations
co-occur within your document. Citations that appear close together are 
connected, with the visualization providing insights into citation patterns
and relationships.

### Creating the Network

```{r network-create, fig.width=8, fig.height=5}
# Create interactive citation network
network <- create_citation_network(
  citation_analysis_results = analysis,
  max_distance = 800,          # Maximum distance in characters
  min_connections = 2,          # Minimum connections to include a node
  show_labels = TRUE            # Show citation labels
)

# Display the network
network
```

### Understanding Network Features

The network visualization includes several visual elements:

- **Node size**: Larger nodes have more connections
- **Node color**: Indicates the primary section where the citation appears
- **Node border**: Thicker borders (3px) indicate citations appearing in multiple sections
- **Edge thickness**: Thicker edges connect citations that appear closer together
- **Edge color**: 
  - Red: Very close citations (≤300 characters)
  - Blue: Moderate distance (≤600 characters)  
  - Gray: Distant citations (>600 characters)

### Network Statistics

Access detailed statistics about the network:

```{r network-stats}
# Get network statistics
stats <- attr(network, "stats")

# Network size
cat("Number of nodes:", stats$n_nodes, "\n")
cat("Number of edges:", stats$n_edges, "\n")
cat("Average distance:", stats$avg_distance, "characters\n")
cat("Maximum distance:", stats$max_distance, "characters\n")

# Distribution by section
print(stats$section_distribution)

# Citations appearing in multiple sections
if (nrow(stats$multi_section_citations) > 0) {
  cat("\nCitations appearing in multiple sections:\n")
  print(stats$multi_section_citations)
}

# Color mapping
cat("\nSection colors:\n")
print(stats$section_colors)
```

### Customizing the Network

You can customize the network based on your analysis needs:

```{r network-custom}
# Focus on very close citations only
network_close <- create_citation_network(
  analysis,
  max_distance = 300,
  min_connections = 1
)

# Show only highly connected "hub" citations
network_hubs <- create_citation_network(
  analysis,
  max_distance = 1000,
  min_connections = 5,
  show_labels = TRUE
)

# Clean visualization without labels
network_clean <- create_citation_network(
  analysis,
  max_distance = 800,
  min_connections = 2,
  show_labels = FALSE
)
```

### Interpreting the Network

The citation network can reveal:

1. **Citation clusters**: Groups of related citations that frequently appear together
2. **Hub citations**: Highly connected citations that appear throughout the document
3. **Section patterns**: How citations are distributed across different sections
4. **Co-citation patterns**: Which references are cited together

```{r network-analysis}
# Find hub citations (most connected)
hub_threshold <- quantile(stats$section_distribution$n, 0.75)
cat("Hub citations (top 25%):\n")
print(stats$section_distribution %>% filter(n >= hub_threshold))

# Analyze network density
network_density <- stats$n_edges / (stats$n_nodes * (stats$n_nodes - 1) / 2)
cat("\nNetwork density:", round(network_density, 3), "\n")
```

### Citation Co-occurrence Data

You can also access the raw co-occurrence data:

```{r network-data}
# View raw co-occurrence data
network_data <- analysis$network_data
head(network_data)

# Citations appearing very close together
close_citations <- network_data %>%
  filter(distance < 100)  # Within 100 characters

cat("Number of very close citation pairs:", nrow(close_citations), "\n")
```

## Reference Matching

### Automatic Matching

Citations are automatically matched to references:

```{r matching}
# View matching results
matched <- analysis$citation_references_mapping %>%
  select(citation_text_clean, cite_author, cite_year, 
         ref_full_text, match_confidence)

head(matched)

# Match quality
table(matched$match_confidence)
```

### Manual Reference Parsing

If you don't have a DOI or prefer text parsing:

```{r manual-refs}
refs <- parse_references_section(doc$References)
head(refs)
```

## Text Analysis

### Word Frequencies

```{r word-freq}
# Top 20 most frequent words
head(analysis$word_frequencies, 20)
```

### N-gram Analysis

```{r ngrams}
# Bigrams
head(analysis$ngrams$`2gram`)

# Trigrams
head(analysis$ngrams$`3gram`)
```

### Readability Metrics

Calculate readability indices for the document:

```{r readability}
# Calculate readability for the full text
readability <- calculate_readability_indices(
  doc$Full_text,
  detailed = TRUE
)

print(readability)

# Compare readability across sections
sections_to_analyze <- c("Abstract", "Introduction", "Methods", "Discussion")
readability_by_section <- lapply(sections_to_analyze, function(section) {
  if (section %in% names(doc)) {
    calculate_readability_indices(doc[[section]], detailed = FALSE)
  }
})
names(readability_by_section) <- sections_to_analyze

# View results
do.call(rbind, readability_by_section)
```

## Word Distribution Analysis

Track how specific terms are distributed across the document:

```{r word-dist}
# Terms of interest
terms <- c("random forest", "machine learning", "accuracy", "tree")

# Calculate distribution
dist <- calculate_word_distribution(
  text = doc,
  selected_words = terms,
  use_sections = TRUE
)

# View results
dist %>%
  select(segment_name, word, count, percentage) %>%
  arrange(segment_name, desc(percentage))
```

### Visualization

```{r plot, fig.width=8, fig.height=5}
# Interactive plot
plot_word_distribution(
  dist,
  plot_type = "line",
  show_points = TRUE,
  smooth = TRUE
)

# Area plot
plot_word_distribution(
  dist,
  plot_type = "area"
)
```

## Advanced Examples

### Finding Specific Citations

```{r find-citations}
# Citations to specific author
analysis$citation_references_mapping %>%
  filter(grepl("Breiman", ref_authors, ignore.case = TRUE))

# Citations in Discussion section
analysis$citations %>%
  filter(section == "Introduction") %>%
  select(citation_text, citation_type)
```

### Custom Stopwords

```{r custom-stop}
custom_stops <- c("however", "therefore", "thus", "moreover")

analysis_custom <- analyze_scientific_content(
  text = doc,
  custom_stopwords = custom_stops,
  remove_stopwords = TRUE
)
```

### Segment-based Analysis

For documents without clear sections:

```{r segments, fig.height=5, fig.width=8}
# Divide into 20 equal segments
dist_segments <- calculate_word_distribution(
  text = doc,
  selected_words = terms,
  use_sections = FALSE,
  n_segments = 20
)

plot_word_distribution(dist_segments, smooth=TRUE)
```

## Export Results

### Save to CSV

```{r export}
# Export citations
write.csv(analysis$citations, "citations.csv", row.names = FALSE)

# Export matched references
write.csv(analysis$citation_references_mapping, 
          "matched_citations.csv", row.names = FALSE)

# Export word frequencies
write.csv(analysis$word_frequencies, 
          "word_frequencies.csv", row.names = FALSE)

# Export network statistics
if (!is.null(network)) {
  stats <- attr(network, "stats")
  write.csv(stats$section_distribution, 
            "network_section_distribution.csv", row.names = FALSE)
  if (nrow(stats$multi_section_citations) > 0) {
    write.csv(stats$multi_section_citations,
              "network_multi_section_citations.csv", row.names = FALSE)
  }
}
```

## Workflow for Multiple Papers

```{r batch, eval=FALSE}
# Process multiple papers
papers <- c("paper1.pdf", "paper2.pdf", "paper3.pdf")
dois <- c("10.xxxx/1", "10.xxxx/2", "10.xxxx/3")

results <- list()
networks <- list()

for (i in seq_along(papers)) {
  doc <- pdf2txt_auto(papers[i], n_columns = 2)
  results[[i]] <- analyze_scientific_content(
    doc, 
    doi = dois[i],
    mailto = "your@email.com"
  )
  
  # Create network for each paper
  networks[[i]] <- create_citation_network(
    results[[i]],
    max_distance = 800,
    min_connections = 2
  )
}

# Combine citation counts
citation_counts <- sapply(results, function(x) x$summary$citations_extracted)
names(citation_counts) <- papers

# Compare network statistics
network_stats <- lapply(networks, function(net) {
  stats <- attr(net, "stats")
  c(nodes = stats$n_nodes, 
    edges = stats$n_edges,
    avg_distance = stats$avg_distance)
})

do.call(rbind, network_stats)
```

## Conclusion

The `contentanalysis` package provides a complete toolkit for analyzing
scientific papers:

1.  **Import**: Handle multi-column PDFs with structure preservation
2.  **Extract**: Detect citations in multiple formats
3.  **Match**: Link citations to references automatically
4.  **Analyze**: Word frequencies, n-grams, citation contexts, readability
5.  **Visualize**: Interactive plots of word distributions and citation networks
6.  **Network**: Explore citation co-occurrence patterns

For more information, see the function documentation:

- `?analyze_scientific_content`
- `?create_citation_network`
- `?calculate_readability_indices`
- `?calculate_word_distribution`
